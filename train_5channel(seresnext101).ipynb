{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import random\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torchvision.transforms as v2\n",
    "import timm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.cli import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)  # Python 내장 random 모듈\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)  # 환경변수 설정\n",
    "    np.random.seed(seed)  # NumPy\n",
    "    torch.manual_seed(seed)  # PyTorch CPU 시드 고정\n",
    "    torch.cuda.manual_seed(seed)  # PyTorch GPU 시드 고정\n",
    "    torch.cuda.manual_seed_all(seed)  # 멀티 GPU 환경에서도 시드 고정\n",
    "    torch.backends.cudnn.deterministic = True  # CuDNN 관련 설정\n",
    "    torch.backends.cudnn.benchmark = False  # 동일한 입력 크기의 데이터가 반복될 경우 속도 향상을 위한 벤치마크 모드 비활성화\n",
    "\n",
    "# 사용 예시\n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지 경로 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = pd.read_csv('datasets/val_label.csv')\n",
    "valid_df['data_path'] = valid_df.apply(lambda row: f\"datasets/image_datasets/user{row['subject_id']}_{row['date']}_valid.png\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df['subject_id'] = valid_df['subject_id'].apply(lambda x: f'user{str(x).zfill(2)}_valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>date</th>\n",
       "      <th>Q1</th>\n",
       "      <th>Q2</th>\n",
       "      <th>Q3</th>\n",
       "      <th>S1</th>\n",
       "      <th>S2</th>\n",
       "      <th>S3</th>\n",
       "      <th>S4</th>\n",
       "      <th>data_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user01_valid</td>\n",
       "      <td>2023-08-20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>datasets/image_datasets/user1_2023-08-20_valid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user01_valid</td>\n",
       "      <td>2023-08-21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>datasets/image_datasets/user1_2023-08-21_valid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user01_valid</td>\n",
       "      <td>2023-08-22</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>datasets/image_datasets/user1_2023-08-22_valid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user01_valid</td>\n",
       "      <td>2023-08-23</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>datasets/image_datasets/user1_2023-08-23_valid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user01_valid</td>\n",
       "      <td>2023-08-24</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>datasets/image_datasets/user1_2023-08-24_valid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     subject_id        date  Q1  Q2  Q3  S1  S2  S3  S4  \\\n",
       "0  user01_valid  2023-08-20   1   1   1   0   0   0   0   \n",
       "1  user01_valid  2023-08-21   1   1   1   0   0   1   0   \n",
       "2  user01_valid  2023-08-22   0   1   1   0   1   1   0   \n",
       "3  user01_valid  2023-08-23   0   1   1   0   0   1   0   \n",
       "4  user01_valid  2023-08-24   1   1   1   0   0   1   0   \n",
       "\n",
       "                                           data_path  \n",
       "0  datasets/image_datasets/user1_2023-08-20_valid...  \n",
       "1  datasets/image_datasets/user1_2023-08-21_valid...  \n",
       "2  datasets/image_datasets/user1_2023-08-22_valid...  \n",
       "3  datasets/image_datasets/user1_2023-08-23_valid...  \n",
       "4  datasets/image_datasets/user1_2023-08-24_valid...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, transforms):\n",
    "        self.path = df['data_path'].values\n",
    "        self.class_ = df[['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3', 'S4']].values\n",
    "        self.transform = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img = np.array(Image.open(self.path[idx]).convert('RGB'))\n",
    "        except FileNotFoundError:\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "        \n",
    "        img = self.transform(image=img)['image']\n",
    "        img = torch.tensor(img, dtype=torch.float)  # Explicitly specify the type as float\n",
    "        y = self.class_[idx]\n",
    "        \n",
    "        return img, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 mean, std 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import cv2\n",
    "# import albumentations as A\n",
    "\n",
    "# # 이미지가 저장된 폴더 경로\n",
    "# folder_path = 'datasets/image_datasets'\n",
    "\n",
    "# # 이미지 파일 목록 가져오기\n",
    "# image_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f)) and f.endswith(\"valid.png\")]\n",
    "\n",
    "# # 모든 이미지를 읽어 numpy 배열에 저장\n",
    "# images = []\n",
    "# for file in image_files:\n",
    "#     image_path = os.path.join(folder_path, file)\n",
    "#     image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # 흑백 이미지로 읽기\n",
    "#     if image is not None:\n",
    "#         images.append(image)\n",
    "\n",
    "# # 이미지 배열을 numpy 배열로 변환\n",
    "# images_array = np.array(images)\n",
    "\n",
    "# # 각 픽셀의 평균과 표준편차 계산\n",
    "# mean = np.mean(images_array)\n",
    "# std = np.std(images_array)\n",
    "\n",
    "# # 결과 출력\n",
    "# print(f\"Mean: {mean}, Std: {std}\")\n",
    "\n",
    "mean = 15.359733188267395\n",
    "std = 58.28444875927197\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "class RandomHorizontalStrips(ImageOnlyTransform):\n",
    "    def __init__(self, num_strips=(1, 3), strip_width=84, always_apply=False, p=0.5):\n",
    "        super(RandomHorizontalStrips, self).__init__(always_apply, p)\n",
    "        self.num_strips = num_strips\n",
    "        self.strip_width = strip_width\n",
    "\n",
    "    def apply(self, img, **params):\n",
    "        h, w = img.shape[:2]\n",
    "        num_strips = np.random.randint(self.num_strips[0], self.num_strips[1] + 1)\n",
    "        for _ in range(num_strips):\n",
    "            x_start = np.random.randint(0, w - self.strip_width)\n",
    "            img[:, x_start:x_start + self.strip_width] = 0\n",
    "        return img\n",
    "\n",
    "    def get_transform_init_args_names(self):\n",
    "        return (\"num_strips\", \"strip_width\")\n",
    "\n",
    "    def get_params(self):\n",
    "        return {\"num_strips\": self.num_strips, \"strip_width\": self.strip_width}\n",
    "\n",
    "# 기존의 train_transforms에 새로운 증강 기법 추가\n",
    "train_transforms = A.Compose([\n",
    "    # RandomHorizontalStrips(p=0.5),\n",
    "    A.Resize(height=224, width=224, p=1),\n",
    "    A.GaussNoise(p=0.5),\n",
    "    A.OneOf([\n",
    "        A.GaussianBlur(p=0.5),\n",
    "        A.Sharpen(p=0.5)\n",
    "    ], p=0.5),\n",
    "    A.Normalize(mean=[mean, mean, mean], std=[std, std, std], p=1.0),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "\n",
    "test_transforms = A.Compose([\n",
    "    A.Resize(always_apply = True, p=1.0, height=224, width=224),\n",
    "    A.Normalize(mean=[mean, mean, mean], std=[std, std, std], p=1.0),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "valid_dataset = CustomDataset(valid_df, test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "def run_model(model, loader, loss_fn=None, optimizer=None, is_training=False, epoch=None):\n",
    "    targets = []\n",
    "    preds = []\n",
    "    smooth_loss_queue = deque(maxlen=50)  # 최근 50개의 손실을 저장할 큐\n",
    "\n",
    "    if is_training:\n",
    "        model.train()\n",
    "        mode = 'Train'\n",
    "    else:\n",
    "        model.eval()\n",
    "        mode = 'Valid/Test'\n",
    "\n",
    "    running_loss = 0.0\n",
    "    bar = tqdm(loader, ascii=True, leave=False)\n",
    "    for cnt, (data, target) in enumerate(bar):\n",
    "        data = data.to('cuda')\n",
    "        target = target.to('cuda')\n",
    "        if is_training:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        outputs = torch.sigmoid(model(data))\n",
    "        total_loss = loss_fn(outputs, target.float())\n",
    "        running_loss += total_loss.item()\n",
    "        smooth_loss_queue.append(total_loss.item())  # 현재 손실을 큐에 추가\n",
    "        smooth_loss = sum(smooth_loss_queue) / len(smooth_loss_queue)  # 큐에 있는 손실의 평균 계산\n",
    "\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        preds.extend(predicted.detach().cpu().tolist())\n",
    "        targets.extend(target.detach().cpu().tolist())\n",
    "\n",
    "        if is_training:\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # 에폭 정보와 함께 배치별 손실 평균 및 smooth loss 출력\n",
    "        bar.set_description(f'Epoch {epoch} {mode} - Loss: {total_loss:.4f}, Smooth Loss: {smooth_loss:.4f}')\n",
    "\n",
    "    f1_score_ = f1_score(np.array(targets), np.array(preds), average='macro')\n",
    "    acc_score = accuracy_score(np.array(targets).reshape(-1), np.array(preds).reshape(-1))\n",
    "\n",
    "    return running_loss / len(loader), acc_score, f1_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.read_csv('datasets/val_label.csv')\n",
    "full_df['data_path'] = full_df.apply(lambda row: f\"datasets/image_datasets/user{row['subject_id']}_{row['date']}_valid.png\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 Train - Loss: 0.2867, Smooth Loss: 0.2734:  50% 3/6 [00:01<00:01,  1.88it/s]       "
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import BCELoss\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import KFold\n",
    "import timm\n",
    "\n",
    "# Configure logging\n",
    "log_path = './logs/seresnext101_32x4d.log'\n",
    "logging.basicConfig(filename=log_path, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# 전체 데이터셋 준비\n",
    "full_dataset = CustomDataset(full_df, train_transforms)  # full_df는 전체 데이터프레임을 나타냅니다.\n",
    "\n",
    "# KFold 설정\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=1020)\n",
    "\n",
    "# K-fold 교차 검증 시작\n",
    "fold_perf = {}\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(full_dataset)):\n",
    "    logging.info(f\"Starting Fold {fold+1}\")\n",
    "    \n",
    "    # 데이터셋 분할\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    valid_subsampler = torch.utils.data.SubsetRandomSampler(valid_idx)\n",
    "    \n",
    "    # 데이터 로더 설정\n",
    "    train_loader = DataLoader(full_dataset, batch_size=16, sampler=train_subsampler)\n",
    "    valid_loader = DataLoader(full_dataset, batch_size=16, sampler=valid_subsampler)\n",
    "    \n",
    "    # 모델 초기화 및 이동\n",
    "    model = timm.create_model('resnext101_32x32d', pretrained=True, num_classes=7)\n",
    "    model = model.to('cuda')\n",
    "    \n",
    "    # 손실 함수, 최적화, 스케줄러 설정\n",
    "    criterion = BCELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=1, eta_min=0.00007)\n",
    "    \n",
    "    best_f1 = -float('inf')\n",
    "    best_loss = 0\n",
    "\n",
    "    # 학습 및 검증\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc, train_f1 = run_model(model, train_loader, criterion, optimizer, is_training=True, epoch=epoch)\n",
    "        valid_loss, valid_acc, valid_f1 = run_model(model, valid_loader, criterion, optimizer, is_training=False, epoch=epoch)\n",
    "        \n",
    "        logging.info(f'Epoch {epoch+1}: Train Loss: {train_loss:.6f}, Train Acc: {train_acc:.6f}, Train F1: {train_f1:.6f}, Valid Loss: {valid_loss:.6f}, Valid Acc: {valid_acc:.6f}, Valid F1: {valid_f1:.6f}')\n",
    "        \n",
    "        if valid_f1 > best_f1:\n",
    "            best_f1 = valid_f1\n",
    "            best_loss = valid_loss\n",
    "            model_path = f'./models/model_fold_{fold}_f1-{best_f1:.3f}_loss-{best_loss:.3f}_resnext101_32x32d.pt'\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            logging.info(f'New best F1-score {valid_f1:.4f} achieved at epoch {epoch}, model saved at {model_path}')\n",
    "        \n",
    "        scheduler.step()\n",
    "    \n",
    "    # 각 fold의 성능 기록\n",
    "    fold_perf[fold] = {\n",
    "        'train_loss': train_loss,\n",
    "        'train_acc': train_acc,\n",
    "        'train_f1': train_f1,\n",
    "        'valid_loss': valid_loss,\n",
    "        'valid_acc': valid_acc,\n",
    "        'valid_f1': valid_f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.1 (NGC 23.07/Python 3.10) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
